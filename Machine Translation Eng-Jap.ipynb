{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077821ea-2f46-435f-9478-517a75d82705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee8d8c-3731-4c0e-ba5b-738df348fe86",
   "metadata": {},
   "source": [
    "!pip install --ignore-installed --user --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3b14cc-c118-431c-90c9-d5db2ee9ff10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c79fde-7745-403b-90a8-828d6b3e0c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a GPU with the name: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(\"Found a GPU with the name:\", gpu.name)\n",
    "else:\n",
    "    print(\"Failed to detect a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b347c4f-c2ed-421e-8714-a5425c428905",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(r\"C:\\Users\\Arvind Vasa\\Downloads\\jpn-eng\\jpn.txt\", delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09b5bd2-d214-40ad-9cf9-7e48b01c849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bfa128c-f4f8-4445-ae95-2fc7327233ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>行け。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>行きなさい。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>こんにちは。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>もしもし。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>やっほー。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0       1\n",
       "0  Go.     行け。\n",
       "1  Go.  行きなさい。\n",
       "2  Hi.  こんにちは。\n",
       "3  Hi.   もしもし。\n",
       "4  Hi.   やっほー。"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0086259f-7442-4b82-b0a1-4c2ff1f52003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106516, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13558451-8566-4fd6-8b8f-181256c45c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns= {0:'English', 1:'Japanese'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5d6db6-83fb-4a66-8ca6-f35df0caa684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Japanese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>行け。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>行きなさい。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>こんにちは。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>もしもし。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>やっほー。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English Japanese\n",
       "0     Go.      行け。\n",
       "1     Go.   行きなさい。\n",
       "2     Hi.   こんにちは。\n",
       "3     Hi.    もしもし。\n",
       "4     Hi.    やっほー。"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45bed880-5cbf-48fe-b819-bc58ec317e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['English']= data['English'].astype(str)\n",
    "data['Japanese']= data['Japanese'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbe7977c-a69c-4fbc-a525-45849c209605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 106516 entries, 0 to 106515\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   English   106516 non-null  object\n",
      " 1   Japanese  106516 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f42ba4-22ec-4918-bb3b-0343f251a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def clean_string(string):\n",
    "    # Replace no-break space with space\n",
    "    string = string.replace(\"\\u202f\",\" \")\n",
    "    # Converts all uppercase characters into lowercase characters\n",
    "    string = string.lower()\n",
    "\n",
    "    # Delete the punctuation and the numbers\n",
    "    for p in punctuation + \"«»\" + \"0123456789\":\n",
    "        string = string.replace(p,\" \")\n",
    "\n",
    "    # Eliminate duplicate whitespaces using wildcards\n",
    "    string = re.sub(\"\\s+\",\" \", string)\n",
    "    # Remove spaces at the beginning and at the end of the string\n",
    "    string = string.strip()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8ea5f68-0ef5-48cc-9265-2344fda220ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['English'] = data['English'].apply(lambda x: clean_string(x))\n",
    "data['Japanese'] = data['Japanese'].apply(lambda x: clean_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773d8ba9-7cdd-495d-be04-97a22975c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data= data.iloc[:15000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06429fb3-2c39-467a-8916-c8f8694ab7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106516, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b2e6bd-c3e1-48df-849c-2d942752b556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Japanese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>行け。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go</td>\n",
       "      <td>行きなさい。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi</td>\n",
       "      <td>こんにちは。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi</td>\n",
       "      <td>もしもし。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi</td>\n",
       "      <td>やっほー。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English Japanese\n",
       "0      go      行け。\n",
       "1      go   行きなさい。\n",
       "2      hi   こんにちは。\n",
       "3      hi    もしもし。\n",
       "4      hi    やっほー。"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bfcc6ac6-5e51-472d-b397-a9bf8b74ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences= data['English'].values\n",
    "target_sentences= data['Japanese'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a2f56069-1ca9-46f5-836d-94d01c3f03cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10651"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_split= int(source_sentences.shape[0] * 0.1 )\n",
    "train_data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed0d24fa-873e-4a68-bc06-bb7e5cff0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences, source_val_sentences= source_sentences[train_data_split:], source_sentences[:train_data_split]\n",
    "target_sentences, target_val_sentences= target_sentences[train_data_split:], target_sentences[:train_data_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6bc5e55-f265-4e32-88ba-462e07044129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_target_sentences(sentences):\n",
    "  tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)\n",
    "  return list(tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67cbabcd-8723-4fb7-bd30-83c6ce5f3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences= tag_target_sentences(target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26078fcb-36d9-47ce-bc00-a062e3569450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> 紙が必要だ。 <eos>',\n",
       " '<sos> 家に帰らなくちゃ。 <eos>',\n",
       " '<sos> 家に帰らなきゃ。 <eos>',\n",
       " '<sos> 温かい水が欲しい。 <eos>',\n",
       " '<sos> 黙ってうなずきました。 <eos>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb8ae3de-e83e-4121-a7cc-81e9896ce6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96516"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab77cf-f57d-4080-a8c9-9ea3aeb45b42",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e50629d-4327-4f2a-beba-ea0778a2581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE Tokenization\n",
    "source_tokenizer= tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\", filters=\"#$%&()*+,-./:;=@[\\\\]^_`{|}~\\t\\n\")\n",
    "source_tokenizer.fit_on_texts(source_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45c8e854-d5a8-4012-a1aa-98f20723bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_inputs = source_tokenizer.texts_to_sequences(source_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e3e44f5-346d-4590-b924-82f09ada01a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11191"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_vocab_size= len(source_tokenizer.word_index) + 1\n",
    "source_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76cc1f39-1a0a-4a16-841e-a06f80ba17d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 169, 45, 93], [2, 169, 73, 99], [2, 231, 4302], [2, 231, 4302], [2, 231, 6, 195, 111]]\n",
      "['i thought as much', 'i thought so too', 'i took highway', 'i took highway', 'i took a week off']\n"
     ]
    }
   ],
   "source": [
    "print(train_encoder_inputs[60:65])\n",
    "print(source_tokenizer.sequences_to_texts(train_encoder_inputs[60:65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dde77ba-6d45-422d-94ef-2c1c513a21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET Tokenization\n",
    "target_tokenizer= tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\", filters=\"#$%&()*+,-./:;=@[\\\\]^_`{|}~\\t\\n\")\n",
    "target_tokenizer.fit_on_texts(target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "819434e8-ad88-4832-a8fe-a533b9628b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82357"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vocab_size= len(target_tokenizer.word_index) + 1\n",
    "target_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95bfd0d9-0479-4f34-97ec-2d7d22fa058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decoder_inputs_targets(sequences, tokenizer):\n",
    "    seqs= tokenizer.texts_to_sequences(sequences)\n",
    "    decoder_inputs= [s[:-1] for s in seqs] # Drops last token in sequence; eg = <sos> Hi I am Arvind\n",
    "    decoder_outputs= [s[1:] for s in seqs] # Drops first token in sequence; eg = Hi I am Arvind <eos>\n",
    "\n",
    "    return decoder_inputs, decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aaecf90d-f18f-47af-86d0-e2a963caf165",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decoder_inputs, train_decoder_outputs= generate_decoder_inputs_targets(target_sentences, target_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cea2629-d600-44ea-b2f0-c171c6edbe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> 紙が必要だ。',\n",
       " '<sos> 家に帰らなくちゃ。',\n",
       " '<sos> 家に帰らなきゃ。',\n",
       " '<sos> 温かい水が欲しい。',\n",
       " '<sos> 黙ってうなずきました。']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder inputs \n",
    "target_tokenizer.sequences_to_texts(train_decoder_inputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cb9ebb5-6022-47d8-a7a5-80cb008755f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['紙が必要だ。 <eos>',\n",
       " '家に帰らなくちゃ。 <eos>',\n",
       " '家に帰らなきゃ。 <eos>',\n",
       " '温かい水が欲しい。 <eos>',\n",
       " '黙ってうなずきました。 <eos>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder outputs\n",
    "target_tokenizer.sequences_to_texts(train_decoder_outputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a57c965-3372-4537-83d6-a0004c7d973c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_encoding_len= len(max(train_encoder_inputs, key= len))\n",
    "max_encoding_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22536604-06f3-429e-bbd7-8655b0d75f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_decoding_len= len(max(train_decoder_inputs, key= len))\n",
    "max_decoding_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044030cb-e00a-4a3f-9584-d86f041f7177",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "957abfc6-af6a-4274-9d31-26b098dce537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_train_encoder_inputs= pad_sequences(train_encoder_inputs, maxlen= max_encoding_len, padding= \"post\", truncating= \"post\")\n",
    "padded_train_decoder_inputs= pad_sequences(train_decoder_inputs, maxlen= max_decoding_len, padding= \"post\", truncating= \"post\")\n",
    "padded_train_decoder_outputs= pad_sequences(train_decoder_outputs, maxlen= max_decoding_len, padding= \"post\", truncating= \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d57a293-33c0-4cc3-bd90-913368f73daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 2375   68  560    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n",
      "[    2 11393     0     0     0     0     0     0     0]\n",
      "[11393     3     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(padded_train_encoder_inputs[10])\n",
    "print(padded_train_decoder_inputs[10])\n",
    "print(padded_train_decoder_outputs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0783f28d-e83c-4943-a5e5-382e7571084b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don t count your chickens before they hatch <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 is considered as OOV and given <unk> value\n",
    "source_tokenizer.sequences_to_texts([padded_train_encoder_inputs[80000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f9506-06c0-4abc-8746-97c55b319496",
   "metadata": {},
   "source": [
    "### validation data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "425354ff-c555-4692-b86b-d21f2ddae3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(preprocessed_input, preprocessed_output):\n",
    "    \n",
    "    tagged_preprocessed_output = tag_target_sentences(preprocessed_output)\n",
    "    # Vectorize encoder source sentences.\n",
    "    encoder_inputs = source_tokenizer.texts_to_sequences(preprocessed_input)\n",
    "    # Vectorize and create decoder input and target sentences.\n",
    "    decoder_inputs, decoder_targets = generate_decoder_inputs_targets(tagged_preprocessed_output, \n",
    "                                                                    target_tokenizer)\n",
    "  \n",
    "    # Pad all collections.\n",
    "    padded_encoder_inputs = pad_sequences(encoder_inputs, max_encoding_len, padding='post', truncating='post')\n",
    "    padded_decoder_inputs = pad_sequences(decoder_inputs, max_decoding_len, padding='post', truncating='post')\n",
    "    padded_decoder_targets = pad_sequences(decoder_targets, max_decoding_len, padding='post', truncating='post')\n",
    "\n",
    "    return padded_encoder_inputs, padded_decoder_inputs, padded_decoder_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b8488bf-1d32-409f-ac30-0c3aa9b18b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process validation dataset\n",
    "padded_val_encoder_inputs, padded_val_decoder_inputs, padded_val_decoder_targets= process_dataset(source_val_sentences, target_val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bdd1e5b-a2e6-40be-8252-f23d505ed791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> 始めていい？ <unk> <unk> <unk> <unk> <unk> <unk> <unk>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokenizer.sequences_to_texts([padded_val_decoder_inputs[1780]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e8e09-4e23-48eb-9fcc-953fcf2896ab",
   "metadata": {},
   "source": [
    "### Building Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17de62d8-8af4-421f-a381-9fd1db8567e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim= 128\n",
    "hidden_dim= 256\n",
    "default_dropout= 0.2\n",
    "batch_size= 32\n",
    "epochs= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66b0b22e-5fda-45d0-9967-c5e2a2fda541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "encoder_inputs= Input(shape= [None], name= 'encoder_inputs')\n",
    "#  mask_zero: Boolean, whether or not the input value 0 is a special\n",
    "encoder_embeddings= Embedding(input_dim= source_vocab_size, output_dim= embedding_dim, mask_zero= True, name= \"encoder_embeddings\")\n",
    "encoder_embedding_outputs= encoder_embeddings(encoder_inputs)\n",
    "\n",
    "encoder_lstm= LSTM(units= hidden_dim, return_state= True, dropout= default_dropout, name= \"encoder_lstm\")\n",
    "# since return sequences is false: the value of encoder_outputs is same as state_h, \n",
    "# if true, all y_hat values of every time stamp is returned to encoder_outputs\n",
    "encoder_outputs, state_h, state_c= encoder_lstm(encoder_embedding_outputs)\n",
    "\n",
    "encoder_states= (state_h, state_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c30b3ad-2b8c-45a2-b1bf-7f2d4e722078",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs= Input(shape= [None], name= \"decoder_inputs\")\n",
    "decoder_embeddings= Embedding(input_dim= target_vocab_size, output_dim= embedding_dim, mask_zero= True, name= \"decoder_embeddings\")\n",
    "decoder_embedding_outputs= decoder_embeddings(decoder_inputs)\n",
    "\n",
    "decoder_lstm= LSTM(units= hidden_dim, return_sequences=True ,return_state= True, dropout= default_dropout, name= \"decoder_lstm\")\n",
    "\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_embedding_outputs, initial_state= encoder_states)\n",
    "\n",
    "decoder_dense= Dense(target_vocab_size, activation='softmax', name=\"decoder_dense\")\n",
    "\n",
    "y_proba= decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01947580-48d4-4d2b-a415-f8ef162085d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"eng_jap_seq2seq_nmt_no_attention\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder_embeddings (Embedding)  (None, None, 128)   1432448     ['encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " decoder_embeddings (Embedding)  (None, None, 128)   10541696    ['decoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 256),        394240      ['encoder_embeddings[0][0]']     \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 256),  394240      ['decoder_embeddings[0][0]',     \n",
      "                                 (None, 256),                     'encoder_lstm[0][1]',           \n",
      "                                 (None, 256)]                     'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)          (None, None, 82357)  21165749    ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33,928,373\n",
      "Trainable params: 33,928,373\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Note how the model is taking two inputs in an array.\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], y_proba, name='eng_jap_seq2seq_nmt_no_attention')\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "516c9212-f288-408c-b7b3-1ddb776bb939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='eng_jap_seq2seq_nmt_no_attention.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48963087-a389-46b1-8fc0-d988ecbe33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this to a folder on my local machine.\n",
    "filepath=\"./HunEngNMTNoAttention/training1/cp.ckpt\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2756954-f4f1-4511-afad-e20df50abc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3016/3017 [============================>.] - ETA: 0s - loss: 1.3377 - accuracy: 0.4968\n",
      "Epoch 1: saving model to ./HunEngNMTNoAttention/training1\\cp.ckpt\n",
      "3017/3017 [==============================] - 295s 93ms/step - loss: 1.3377 - accuracy: 0.4968 - val_loss: 1.3714 - val_accuracy: 0.4994\n",
      "Epoch 2/3\n",
      "3016/3017 [============================>.] - ETA: 0s - loss: 1.2820 - accuracy: 0.4973\n",
      "Epoch 2: saving model to ./HunEngNMTNoAttention/training1\\cp.ckpt\n",
      "3017/3017 [==============================] - 274s 91ms/step - loss: 1.2820 - accuracy: 0.4973 - val_loss: 1.4570 - val_accuracy: 0.4996\n",
      "Epoch 3/3\n",
      "3017/3017 [==============================] - ETA: 0s - loss: 1.2717 - accuracy: 0.4973\n",
      "Epoch 3: saving model to ./HunEngNMTNoAttention/training1\\cp.ckpt\n",
      "3017/3017 [==============================] - 275s 91ms/step - loss: 1.2717 - accuracy: 0.4973 - val_loss: 1.5449 - val_accuracy: 0.4994\n"
     ]
    }
   ],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit([padded_train_encoder_inputs, padded_train_decoder_inputs], padded_train_decoder_outputs,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=3,\n",
    "                     validation_data=([padded_val_encoder_inputs, padded_val_decoder_inputs], padded_val_decoder_targets),\n",
    "                     callbacks=[cp_callback, es_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989b073-153b-4047-9df5-2ce4940e0cc6",
   "metadata": {},
   "source": [
    "### Saving Model and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "210d6c4e-c702-4ce9-b397-82e87582ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "from keras.models import load_model\n",
    "\n",
    "# Save model to HDF5 format\n",
    "model.save('artifacts/eng_jap_seq2seq_nmt_no_attention.h5')\n",
    "\n",
    "# Load model from HDF5 format\n",
    "loaded_model = load_model('artifacts/eng_jap_seq2seq_nmt_no_attention.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cc1d5d1a-390b-467c-81a1-1ba41ea04fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model.fit([padded_train_encoder_inputs, padded_train_decoder_inputs], padded_train_decoder_outputs,\n",
    "#                      batch_size=batch_size,\n",
    "#                      epochs=3,\n",
    "#                      validation_data=([padded_val_encoder_inputs, padded_val_decoder_inputs], padded_val_decoder_targets),\n",
    "#                      callbacks=[cp_callback, es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6f965f0-8380-4701-b2e1-ad7fd45ebb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Tokenizer\n",
    "import io\n",
    "import json\n",
    "\n",
    "\n",
    "##### Save the tokenizers as JSON files. The resulting files can be downloaded by left-clicking on them.\n",
    "source_tokenizer_json = source_tokenizer.to_json()\n",
    "with io.open('artifacts/source_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "  f.write(json.dumps(source_tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "target_tokenizer_json = target_tokenizer.to_json()\n",
    "with io.open('artifacts/target_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "  f.write(json.dumps(target_tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "427b272c-fb78-4ea6-90cc-4ba084296399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Tokenizer\n",
    "with io.open('artifacts/source_tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "    # Read the JSON data\n",
    "    st= json.load(f)\n",
    "    st = tf.keras.preprocessing.text.tokenizer_from_json(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b79389-e5b6-4dc2-a3ca-da0213256c38",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1406c72-3c32-44b0-8029-024421c8d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 12s 38ms/step - loss: 1.5449 - accuracy: 0.4994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5448741912841797, 0.4994009733200073]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test set.\n",
    "model.evaluate([padded_val_encoder_inputs, padded_val_decoder_inputs], padded_val_decoder_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "92d1a289-5edc-4b7b-8b5f-8aa373e51e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder_inputs',\n",
       " 'decoder_inputs',\n",
       " 'encoder_embeddings',\n",
       " 'decoder_embeddings',\n",
       " 'encoder_lstm',\n",
       " 'decoder_lstm',\n",
       " 'decoder_dense']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[layer.name for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5af5c2b0-e63f-4eac-816f-b823c76de1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = model.get_layer('encoder_inputs').input\n",
    "\n",
    "encoder_embedding_layer = model.get_layer('encoder_embeddings')\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "encoder_lstm = model.get_layer('encoder_lstm')\n",
    "\n",
    "_, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embeddings)\n",
    "\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "encoder_model_no_attention = tf.keras.Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3c395b2d-7038-461f-944b-c3a824c5c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = model.get_layer('decoder_inputs').input\n",
    "\n",
    "decoder_embedding_layer = model.get_layer('decoder_embeddings')\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# Inputs to represent the decoder's LSTM hidden and cell states. We'll populate \n",
    "# these manually using the encoder's output for the initial state.\n",
    "decoder_input_state_h = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_h')\n",
    "decoder_input_state_c = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_c')\n",
    "decoder_input_states = [decoder_input_state_h, decoder_input_state_c]\n",
    "\n",
    "decoder_lstm = model.get_layer('decoder_lstm')\n",
    "\n",
    "decoder_sequence_outputs, decoder_output_state_h, decoder_output_state_c = decoder_lstm(\n",
    "    decoder_embeddings, initial_state=decoder_input_states\n",
    ")\n",
    "\n",
    "# Update hidden and cell states for the next time step.\n",
    "decoder_output_states = [decoder_output_state_h, decoder_output_state_c]\n",
    "\n",
    "decoder_dense = model.get_layer('decoder_dense')\n",
    "y_proba = decoder_dense(decoder_sequence_outputs)\n",
    "\n",
    "decoder_model_no_attention = tf.keras.Model(\n",
    "    [decoder_inputs] + decoder_input_states, \n",
    "    [y_proba] + decoder_output_states\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a4c39fa-3d3d-417a-8819-e04bf02706ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_without_attention(sentence: str, \n",
    "                                source_tokenizer, encoder,\n",
    "                                target_tokenizer, decoder,\n",
    "                                max_translated_len = 30):\n",
    "\n",
    "  # Vectorize the source sentence and run it through the encoder.    \n",
    "  input_seq = source_tokenizer.texts_to_sequences([sentence])\n",
    "\n",
    "  # Get the tokenized sentence to see if there are any unknown tokens.\n",
    "  tokenized_sentence = source_tokenizer.sequences_to_texts(input_seq)\n",
    "\n",
    "  states = encoder.predict(input_seq)  \n",
    "\n",
    "  current_word = '<sos>'\n",
    "  decoded_sentence = []\n",
    "\n",
    "  while len(decoded_sentence) < max_translated_len:\n",
    "    \n",
    "    # Set the next input word for the decoder.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_tokenizer.word_index[current_word]\n",
    "    \n",
    "    # Determine the next word.\n",
    "    target_y_proba, h, c = decoder.predict([target_seq] + states)\n",
    "    target_token_index = np.argmax(target_y_proba[0, -1, :])\n",
    "    current_word = target_tokenizer.index_word[target_token_index]\n",
    "\n",
    "    if (current_word == '<eos>'):\n",
    "      break\n",
    "\n",
    "    decoded_sentence.append(current_word)\n",
    "    states = [h, c]\n",
    "  \n",
    "  return tokenized_sentence[0], ' '.join(decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "33c842a9-9aef-40e3-a30b-09ba2dd32e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "source_sentences= source_val_sentences[600:650]\n",
    "target_sentences= target_val_sentences[600:650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "294c253d-6d43-4ca2-80a0-ba1c73cfefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentences(source_sentences, target_sentences,translation_func, source_tokenizer, encoder,\n",
    "                        target_tokenizer, decoder):\n",
    "  translations = {'Tokenized Original': [], 'Reference': [], 'Translation': []}\n",
    "\n",
    "  for s in source_sentences:\n",
    "    tokenized_sentence, translated = translation_func(s, source_tokenizer, encoder,\n",
    "                                                      target_tokenizer, decoder)\n",
    "\n",
    "    translations['Tokenized Original'].append(tokenized_sentence)\n",
    "    translations['Translation'].append(translated)\n",
    "\n",
    "  for t in target_sentences:\n",
    "      translations['Reference'].append(t)\n",
    "  \n",
    "  return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "03cd9531-d70e-4171-ba0d-1fd6b80a0d92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized Original</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i promise</td>\n",
       "      <td>約束するよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i relaxed</td>\n",
       "      <td>僕はリラックスした。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i said no</td>\n",
       "      <td>ダメだって言ったでしょ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i said no</td>\n",
       "      <td>違うってば。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i said so</td>\n",
       "      <td>私はそう言いました。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i said so</td>\n",
       "      <td>そう言っておいたはずだ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i saw him</td>\n",
       "      <td>私は彼に会った。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i saw him</td>\n",
       "      <td>彼を見た。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i see tom</td>\n",
       "      <td>トムが見える。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i want it</td>\n",
       "      <td>これが欲しい。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i was mad</td>\n",
       "      <td>私は怒り狂っていたんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i will go</td>\n",
       "      <td>行くよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i will go</td>\n",
       "      <td>私が行きます。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i d agree</td>\n",
       "      <td>私は同意するだろう。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i ll help</td>\n",
       "      <td>お手伝いしますよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i ll help</td>\n",
       "      <td>私は手伝うよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i ll help</td>\n",
       "      <td>手伝うよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i ll pass</td>\n",
       "      <td>私は通ります。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>i ll pass</td>\n",
       "      <td>やめときます。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>i ll quit</td>\n",
       "      <td>私はやめます。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>i m a boy</td>\n",
       "      <td>僕、男だよ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>i m a cop</td>\n",
       "      <td>私は警官だ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i m alone</td>\n",
       "      <td>一人だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i m alone</td>\n",
       "      <td>私は一人だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>i m awake</td>\n",
       "      <td>目は覚めています。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>i m awake</td>\n",
       "      <td>起きてるよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>i m bored</td>\n",
       "      <td>退屈しちゃったよ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>i m brave</td>\n",
       "      <td>私は勇敢だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>i m broke</td>\n",
       "      <td>文無しなんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>i m broke</td>\n",
       "      <td>金欠なんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>i m clean</td>\n",
       "      <td>私は潔白だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>i m clean</td>\n",
       "      <td>私は清潔だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>i m dizzy</td>\n",
       "      <td>くらくらする。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>i m dizzy</td>\n",
       "      <td>酔った。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>i m drunk</td>\n",
       "      <td>酔った。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>i m dying</td>\n",
       "      <td>もう死にそう。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>i m early</td>\n",
       "      <td>早かった。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>i m first</td>\n",
       "      <td>私が一番。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>i m fussy</td>\n",
       "      <td>私は几帳面なんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>i m fussy</td>\n",
       "      <td>私は細かいんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>i m happy</td>\n",
       "      <td>私は幸福です。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>i m happy</td>\n",
       "      <td>私は幸せだ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>i m happy</td>\n",
       "      <td>私は幸せです。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>i m lucky</td>\n",
       "      <td>私はついている。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>i m picky</td>\n",
       "      <td>私は細かいんだ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>i m ready</td>\n",
       "      <td>準備ができました。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>i m smart</td>\n",
       "      <td>俺って、頭いい！</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>i m smart</td>\n",
       "      <td>私って冴えてる！</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>i m sober</td>\n",
       "      <td>私は酔っていない。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>i m sorry</td>\n",
       "      <td>悪かった。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tokenized Original     Reference  \\\n",
       "0           i promise        約束するよ。   \n",
       "1           i relaxed    僕はリラックスした。   \n",
       "2           i said no  ダメだって言ったでしょ。   \n",
       "3           i said no        違うってば。   \n",
       "4           i said so    私はそう言いました。   \n",
       "5           i said so  そう言っておいたはずだ。   \n",
       "6           i saw him      私は彼に会った。   \n",
       "7           i saw him         彼を見た。   \n",
       "8           i see tom       トムが見える。   \n",
       "9           i want it       これが欲しい。   \n",
       "10          i was mad  私は怒り狂っていたんだ。   \n",
       "11          i will go          行くよ。   \n",
       "12          i will go       私が行きます。   \n",
       "13          i d agree    私は同意するだろう。   \n",
       "14          i ll help     お手伝いしますよ。   \n",
       "15          i ll help       私は手伝うよ。   \n",
       "16          i ll help         手伝うよ。   \n",
       "17          i ll pass       私は通ります。   \n",
       "18          i ll pass       やめときます。   \n",
       "19          i ll quit       私はやめます。   \n",
       "20          i m a boy        僕、男だよ。   \n",
       "21          i m a cop        私は警官だ。   \n",
       "22          i m alone          一人だ。   \n",
       "23          i m alone        私は一人だ。   \n",
       "24          i m awake     目は覚めています。   \n",
       "25          i m awake        起きてるよ。   \n",
       "26          i m bored     退屈しちゃったよ。   \n",
       "27          i m brave        私は勇敢だ。   \n",
       "28          i m broke       文無しなんだ。   \n",
       "29          i m broke        金欠なんだ。   \n",
       "30          i m clean        私は潔白だ。   \n",
       "31          i m clean        私は清潔だ。   \n",
       "32          i m dizzy       くらくらする。   \n",
       "33          i m dizzy          酔った。   \n",
       "34          i m drunk          酔った。   \n",
       "35          i m dying       もう死にそう。   \n",
       "36          i m early         早かった。   \n",
       "37          i m first         私が一番。   \n",
       "38          i m fussy     私は几帳面なんだ。   \n",
       "39          i m fussy      私は細かいんだ。   \n",
       "40          i m happy       私は幸福です。   \n",
       "41          i m happy        私は幸せだ。   \n",
       "42          i m happy       私は幸せです。   \n",
       "43          i m lucky      私はついている。   \n",
       "44          i m picky      私は細かいんだ。   \n",
       "45          i m ready     準備ができました。   \n",
       "46          i m smart      俺って、頭いい！   \n",
       "47          i m smart      私って冴えてる！   \n",
       "48          i m sober     私は酔っていない。   \n",
       "49          i m sorry         悪かった。   \n",
       "\n",
       "                                          Translation  \n",
       "0                                            トムは 時だよ。  \n",
       "1                                            トムは 時だよ。  \n",
       "2                                            トムは 時だよ。  \n",
       "3                                            トムは 時だよ。  \n",
       "4                                            トムは 時だよ。  \n",
       "5                                            トムは 時だよ。  \n",
       "6                                            トムは 時だよ。  \n",
       "7                                            トムは 時だよ。  \n",
       "8                                            トムは 時だよ。  \n",
       "9                                            トムは 時だよ。  \n",
       "10  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "11                                           トムは 時だよ。  \n",
       "12                                           トムは 時だよ。  \n",
       "13                                           トムは 時だよ。  \n",
       "14                                           トムは 時だよ。  \n",
       "15                                           トムは 時だよ。  \n",
       "16                                           トムは 時だよ。  \n",
       "17                                           トムは 時だよ。  \n",
       "18                                           トムは 時だよ。  \n",
       "19                                           トムは 時だよ。  \n",
       "20  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "21  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "22                                           トムは 時だよ。  \n",
       "23                                           トムは 時だよ。  \n",
       "24                                           トムは 時だよ。  \n",
       "25                                           トムは 時だよ。  \n",
       "26  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "27                                           トムは 時だよ。  \n",
       "28  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "29  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "30                                           トムは 時だよ。  \n",
       "31                                           トムは 時だよ。  \n",
       "32                                           トムは 時だよ。  \n",
       "33                                           トムは 時だよ。  \n",
       "34                                           トムは 時だよ。  \n",
       "35  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "36                                           トムは 時だよ。  \n",
       "37  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "38  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "39  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "40                                           トムは 時だよ。  \n",
       "41                                           トムは 時だよ。  \n",
       "42                                           トムは 時だよ。  \n",
       "43  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "44                                           トムは 時だよ。  \n",
       "45  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "46                                           トムは 時だよ。  \n",
       "47                                           トムは 時だよ。  \n",
       "48  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "49                                           トムは 時だよ。  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations_no_attention = pd.DataFrame(translate_sentences(source_sentences, target_sentences, translate_without_attention,\n",
    "                                                             source_tokenizer, encoder_model_no_attention,\n",
    "                                                             target_tokenizer, decoder_model_no_attention))\n",
    "translations_no_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "87b0ab51-e9e3-4b52-8991-d7bb93e653a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized Original</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i promise</td>\n",
       "      <td>約束するよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i relaxed</td>\n",
       "      <td>僕はリラックスした。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i said no</td>\n",
       "      <td>ダメだって言ったでしょ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i said no</td>\n",
       "      <td>違うってば。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i said so</td>\n",
       "      <td>私はそう言いました。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i said so</td>\n",
       "      <td>そう言っておいたはずだ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i saw him</td>\n",
       "      <td>私は彼に会った。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i saw him</td>\n",
       "      <td>彼を見た。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i see tom</td>\n",
       "      <td>トムが見える。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i want it</td>\n",
       "      <td>これが欲しい。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i was mad</td>\n",
       "      <td>私は怒り狂っていたんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i will go</td>\n",
       "      <td>行くよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i will go</td>\n",
       "      <td>私が行きます。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i d agree</td>\n",
       "      <td>私は同意するだろう。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i ll help</td>\n",
       "      <td>お手伝いしますよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i ll help</td>\n",
       "      <td>私は手伝うよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i ll help</td>\n",
       "      <td>手伝うよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i ll pass</td>\n",
       "      <td>私は通ります。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>i ll pass</td>\n",
       "      <td>やめときます。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>i ll quit</td>\n",
       "      <td>私はやめます。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>i m a boy</td>\n",
       "      <td>僕、男だよ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>i m a cop</td>\n",
       "      <td>私は警官だ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i m alone</td>\n",
       "      <td>一人だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i m alone</td>\n",
       "      <td>私は一人だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>i m awake</td>\n",
       "      <td>目は覚めています。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>i m awake</td>\n",
       "      <td>起きてるよ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>i m bored</td>\n",
       "      <td>退屈しちゃったよ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>i m brave</td>\n",
       "      <td>私は勇敢だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>i m broke</td>\n",
       "      <td>文無しなんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>i m broke</td>\n",
       "      <td>金欠なんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>i m clean</td>\n",
       "      <td>私は潔白だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>i m clean</td>\n",
       "      <td>私は清潔だ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>i m dizzy</td>\n",
       "      <td>くらくらする。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>i m dizzy</td>\n",
       "      <td>酔った。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>i m drunk</td>\n",
       "      <td>酔った。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>i m dying</td>\n",
       "      <td>もう死にそう。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>i m early</td>\n",
       "      <td>早かった。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>i m first</td>\n",
       "      <td>私が一番。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>i m fussy</td>\n",
       "      <td>私は几帳面なんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>i m fussy</td>\n",
       "      <td>私は細かいんだ。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>i m happy</td>\n",
       "      <td>私は幸福です。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>i m happy</td>\n",
       "      <td>私は幸せだ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>i m happy</td>\n",
       "      <td>私は幸せです。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>i m lucky</td>\n",
       "      <td>私はついている。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>i m picky</td>\n",
       "      <td>私は細かいんだ。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>i m ready</td>\n",
       "      <td>準備ができました。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>i m smart</td>\n",
       "      <td>俺って、頭いい！</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>i m smart</td>\n",
       "      <td>私って冴えてる！</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>i m sober</td>\n",
       "      <td>私は酔っていない。</td>\n",
       "      <td>トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>i m sorry</td>\n",
       "      <td>悪かった。</td>\n",
       "      <td>トムは 時だよ。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tokenized Original     Reference  \\\n",
       "0           i promise        約束するよ。   \n",
       "1           i relaxed    僕はリラックスした。   \n",
       "2           i said no  ダメだって言ったでしょ。   \n",
       "3           i said no        違うってば。   \n",
       "4           i said so    私はそう言いました。   \n",
       "5           i said so  そう言っておいたはずだ。   \n",
       "6           i saw him      私は彼に会った。   \n",
       "7           i saw him         彼を見た。   \n",
       "8           i see tom       トムが見える。   \n",
       "9           i want it       これが欲しい。   \n",
       "10          i was mad  私は怒り狂っていたんだ。   \n",
       "11          i will go          行くよ。   \n",
       "12          i will go       私が行きます。   \n",
       "13          i d agree    私は同意するだろう。   \n",
       "14          i ll help     お手伝いしますよ。   \n",
       "15          i ll help       私は手伝うよ。   \n",
       "16          i ll help         手伝うよ。   \n",
       "17          i ll pass       私は通ります。   \n",
       "18          i ll pass       やめときます。   \n",
       "19          i ll quit       私はやめます。   \n",
       "20          i m a boy        僕、男だよ。   \n",
       "21          i m a cop        私は警官だ。   \n",
       "22          i m alone          一人だ。   \n",
       "23          i m alone        私は一人だ。   \n",
       "24          i m awake     目は覚めています。   \n",
       "25          i m awake        起きてるよ。   \n",
       "26          i m bored     退屈しちゃったよ。   \n",
       "27          i m brave        私は勇敢だ。   \n",
       "28          i m broke       文無しなんだ。   \n",
       "29          i m broke        金欠なんだ。   \n",
       "30          i m clean        私は潔白だ。   \n",
       "31          i m clean        私は清潔だ。   \n",
       "32          i m dizzy       くらくらする。   \n",
       "33          i m dizzy          酔った。   \n",
       "34          i m drunk          酔った。   \n",
       "35          i m dying       もう死にそう。   \n",
       "36          i m early         早かった。   \n",
       "37          i m first         私が一番。   \n",
       "38          i m fussy     私は几帳面なんだ。   \n",
       "39          i m fussy      私は細かいんだ。   \n",
       "40          i m happy       私は幸福です。   \n",
       "41          i m happy        私は幸せだ。   \n",
       "42          i m happy       私は幸せです。   \n",
       "43          i m lucky      私はついている。   \n",
       "44          i m picky      私は細かいんだ。   \n",
       "45          i m ready     準備ができました。   \n",
       "46          i m smart      俺って、頭いい！   \n",
       "47          i m smart      私って冴えてる！   \n",
       "48          i m sober     私は酔っていない。   \n",
       "49          i m sorry         悪かった。   \n",
       "\n",
       "                                          Translation  \n",
       "0                                            トムは 時だよ。  \n",
       "1                                            トムは 時だよ。  \n",
       "2                                            トムは 時だよ。  \n",
       "3                                            トムは 時だよ。  \n",
       "4                                            トムは 時だよ。  \n",
       "5                                            トムは 時だよ。  \n",
       "6                                            トムは 時だよ。  \n",
       "7                                            トムは 時だよ。  \n",
       "8                                            トムは 時だよ。  \n",
       "9                                            トムは 時だよ。  \n",
       "10  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "11                                           トムは 時だよ。  \n",
       "12                                           トムは 時だよ。  \n",
       "13                                           トムは 時だよ。  \n",
       "14                                           トムは 時だよ。  \n",
       "15                                           トムは 時だよ。  \n",
       "16                                           トムは 時だよ。  \n",
       "17                                           トムは 時だよ。  \n",
       "18                                           トムは 時だよ。  \n",
       "19                                           トムは 時だよ。  \n",
       "20  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "21  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "22                                           トムは 時だよ。  \n",
       "23                                           トムは 時だよ。  \n",
       "24                                           トムは 時だよ。  \n",
       "25                                           トムは 時だよ。  \n",
       "26  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "27                                           トムは 時だよ。  \n",
       "28  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "29  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "30                                           トムは 時だよ。  \n",
       "31                                           トムは 時だよ。  \n",
       "32                                           トムは 時だよ。  \n",
       "33                                           トムは 時だよ。  \n",
       "34                                           トムは 時だよ。  \n",
       "35  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "36                                           トムは 時だよ。  \n",
       "37  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "38  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "39  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "40                                           トムは 時だよ。  \n",
       "41                                           トムは 時だよ。  \n",
       "42                                           トムは 時だよ。  \n",
       "43  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "44                                           トムは 時だよ。  \n",
       "45  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "46                                           トムは 時だよ。  \n",
       "47                                           トムは 時だよ。  \n",
       "48  トムは 時 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 月 ...  \n",
       "49                                           トムは 時だよ。  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations_no_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b28ddb23-4df0-4876-8713-f8f6917ed3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences= target_tokenizer.texts_to_sequences(target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9fad5fa-015c-457d-b569-fd980b5f6a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82357"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vocab_size= len(target_tokenizer.word_index) + 1\n",
    "target_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e055b-deec-4223-9163-0740cc68bfdd",
   "metadata": {},
   "source": [
    "eng_train_sentences, eng_test_sentences= eng_sentences[:85000], eng_sentences[85000:]\n",
    "jap_train_sentences, jap_test_sentences= jap_sentences[:85000], jap_sentences[85000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbab2199-24b9-438a-b88c-71d2c7ecd2e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eng_train_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43meng_train_sentences\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eng_train_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "eng_train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af3a11-ae91-4ccb-bc72-464d2f06139d",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d2957-95c9-4621-aa08-e721955cf5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f561f44-d63f-428f-9f8b-b1bf53995161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE padding\n",
    "eng_padded_sentences= pad_sequences(eng_train_sentences, padding= 'post')\n",
    "jap_padded_sentences= pad_sequences(jap_train_sentences, padding= 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e722574-4b89-42a2-a2ac-bd56feb1e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_size= len(eng_tokenizer.word_index) + 1\n",
    "jap_vocab_size= len(jap_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb20469-0603-4d9a-8af9-0e81ac2379d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eng_vocab_size)\n",
    "print(jap_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b839fa-fdcd-49fd-aac4-4e6da487d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eng_sent_length= len(eng_padded_sentences[0])\n",
    "max_jap_sent_length= len(jap_padded_sentences[0])\n",
    "max_eng_sent_length, max_jap_sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b101c-db74-4cf7-92ba-0e08425ae4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "embedding_dim= 32\n",
    "units= 256\n",
    "\n",
    "# Define the encoder\n",
    "encoder_input = Input(shape=(max_eng_sent_length,))\n",
    "encoder_embedding = Embedding(input_dim=eng_vocab_size, output_dim=embedding_dim)(encoder_input)\n",
    "encoder_lstm, state_h, state_c = LSTM(units, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_input = Input(shape=(max_jap_sent_length-1,))\n",
    "decoder_embedding = Embedding(input_dim= jap_vocab_size, output_dim=embedding_dim)(decoder_input)\n",
    "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(jap_vocab_size, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e2889-0557-4c1e-bfd5-fee68b698aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([eng_padded_sentences, jap_padded_sentences[:, :-1]], jap_padded_sentences[:, 1:], epochs=3, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee7d11-792d-4031-8f63-0d08b258a3bc",
   "metadata": {},
   "source": [
    "import pickle \n",
    "\n",
    "with open('eng_jap_translator.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5090040-1dee-4d13-a87a-b56e7316b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    inputs = tf.convert_to_tensor(sentence)\n",
    "    result = ''\n",
    "    inputs = tf.expand_dims(inputs, axis=0)\n",
    "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
    "    enc_out, state = encoder(inputs, hidden)\n",
    "    hidden_state = state\n",
    "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
    "    for t in range(max_length_target):\n",
    "        predictions, hidden_state = decoder(dec_input,\n",
    "                                                             hidden_state,\n",
    "                                                             enc_out)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
    "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
    "            return result\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fff8db-ad8f-4c06-91be-fc41fd4e212c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([eng_padded_sentences, jap_padded_sentences[:, :-1]], jap_padded_sentences[:, 1:], epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e17a4-7d91-4356-8af6-aea691536d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37471d3-a494-47e0-b79a-a36ed47f03fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2b233-e4a1-448c-bd42-5e95baab9892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97551c8f-f1e9-4b51-b9c5-16434c497c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d837e0e3-7a09-420c-ae86-ced75f6a44f5",
   "metadata": {},
   "source": [
    "<!-- from tensorflow.keras.utils import plot_model\n",
    "import pydot\n",
    "import graphviz \n",
    "\n",
    "# Your model definition code goes here\n",
    "\n",
    "# Plot the model and save the image \n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a67331-fc99-4e2b-814b-cca7ef0f5334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU-ENV",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
